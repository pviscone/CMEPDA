{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lez  08/11/21 Inizio ML\n",
    "\n",
    "**# VEDI CORSO ML, qui solo aggiunte**\n",
    "\n",
    "- Per classificazione spesso come loss si usa la **binary cross entropy** \n",
    "\n",
    "\n",
    "\n",
    "  $$D_{BCE}= -\\frac{1}{N}\\sum_{i=1}^N [y_i log(\\hat{y_i})+(1-y_i)log(1-\\hat{y_i}))]$$\n",
    "\n",
    "\n",
    "\n",
    "  Minimizzare questa funzione equivale a massimizzare la (log)likelihood di una \"binomiale\" (credo)\n",
    "\n",
    "  $$L=\\prod_i p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "\n",
    "- Conviene sempre la regressione 1 of k poichè altrimenti, per esempio, nel caso di indecisione tra 1 e 4 (se classificazione tra 1,2,3,4) il modello potrebbe torneare 2 che è una classe totalmente separata\n",
    "\n",
    "- Ricordiamo che noi stiamo cercando di imparare la pdf $P(x|y)$\n",
    "\n",
    "  RIVEDI SLIDE 19 SU ANALOGIA SUPERVISED, UNSUPERVISED\n",
    "\n",
    "- CAPACITA' (rizzi) = COMPLESSITÀ (micheli)\n",
    "\n",
    "  parametri(rizzi) = pesi (micheli)\n",
    "\n",
    "- STUDIA DA MICHELI BIAS VARIANCE TRADEOFF\n",
    "\n",
    "## Esempi modelli\n",
    "\n",
    "### PCA\n",
    "\n",
    "Consiste in una trasformazione ortogonale degli input che diagonalizza la matrice di covarianza dei dati, praticamente si cercano i suoi autovettori (è un esempio di modello unsupervised usato per ridurre la dimensionalità dei problemi VEDI MEGLIO ).\n",
    "\n",
    "Spesso è utile anche per normalizzare i dati rispetto alla loro varianza\n",
    "\n",
    "### Manifold learning\n",
    "\n",
    "VEDI\n",
    "\n",
    "### KNN\n",
    "\n",
    "Può essere esteso a regressione. Il principio è che un test point x prende il valore del training point più vicino\n",
    "\n",
    "### Decision tree (Vedi meglio)\n",
    "\n",
    "Le funzioni usate dal modello sono alberi. Ogni nodo contiene una condizione. La profondità dell'albero è un iperparametro del modello.\n",
    "\n",
    "Tipicamente con un solo albero si ottengono perstazioni pessime, quello che si fa è usare ensemble di alberi (delle foreste). Ci sono diversi modi di combinare alberi\n",
    "\n",
    "- Random forest\n",
    "- gradient boosting: ogni albero tenta di correggere gli errori del precedente\n",
    "- adaptive boosting\n",
    "\n",
    "I limiti dei decision tree è che i tagli sono fatti solo lungo gli assi e, nonostante il problema è mitigabile tramite PCA o aumentando il numero di alberi, può essere molto problematico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
