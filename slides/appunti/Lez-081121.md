# Lez  08/11/21 Inizio ML

**# VEDI CORSO ML, qui solo aggiunte**

- Per classificazione spesso come loss si usa la **binary cross entropy** 

  

  $$D_{BCE}= -\frac{1}{N}\sum_{i=1}^N [y_i log(\hat{y_i})+(1-y_i)log(1-\hat{y_i}))]$$

  

  Minimizzare questa funzione equivale a massimizzare la (log)likelihood di una "binomiale" (credo)

  $$L=\prod_i p_i^{y_i}(1-p_i)^{1-y_i}$$

- Conviene sempre la regressione 1 of k poichè altrimenti, per esempio, nel caso di indecisione tra 1 e 4 (se classificazione tra 1,2,3,4) il modello potrebbe torneare 2 che è una classe totalmente separata

- Ricordiamo che noi stiamo cercando di imparare la pdf $P(x|y)$

  RIVEDI SLIDE 19 SU ANALOGIA SUPERVISED, UNSUPERVISED

- CAPACITA' (rizzi) = COMPLESSITÀ (micheli) 

  parametri(rizzi) = pesi (micheli)

- STUDIA DA MICHELI BIAS VARIANCE TRADEOFF

## Esempi modelli

### PCA

Consiste in una trasformazione ortogonale degli input che diagonalizza la matrice di covarianza dei dati, praticamente si cercano i suoi autovettori (è un esempio di modello unsupervised usato per ridurre la dimensionalità dei problemi VEDI MEGLIO ).

Spesso è utile anche per normalizzare i dati rispetto alla loro varianza

### Manifold learning

VEDI

### KNN

Può essere esteso a regressione. Il principio è che un test point x prende il valore del training point più vicino

### Decision tree (Vedi meglio)

Le funzioni usate dal modello sono alberi. Ogni nodo contiene una condizione. La profondità dell'albero è un iperparametro del modello.

Tipicamente con un solo albero si ottengono perstazioni pessime, quello che si fa è usare ensemble di alberi (delle foreste). Ci sono diversi modi di combinare alberi

- Random forest
- gradient boosting: ogni albero tenta di correggere gli errori del precedente
- adaptive boosting

I limiti dei decision tree è che i tagli sono fatti solo lungo gli assi e, nonostante il problema è mitigabile tramite PCA o aumentando il numero di alberi, può essere molto problematico